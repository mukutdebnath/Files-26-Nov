MVM Config:--------------
Bit Stream:  1
Bit Slice:  2
ADC Bit:  4
Xbar size:  128 128
Real ADC Characteristics:  True ideal
Xbar Weight Std Gamma 0.0
WARNING: crossbar sizes with different row annd column dimension not supported.
MVM Config:--------------
Bit Stream:  1
Bit Slice:  2
ADC Bit:  4
Xbar size:  128 128
Real ADC Characteristics:  True ideal
Xbar Weight Std Gamma 0.0
WARNING: crossbar sizes with different row annd column dimension not supported.
Available models:
resnet20_adc resnet110 resnet1202 resnet20 resnet32 resnet44 resnet56 resnet8 resnet20_mvm
Loading pretrained model from  saved_models/resnet20_qwa_nobias_1_12Aug.th
Pretrained model training accuracy: 93.05
-Starting retraining on mvm model, fp model weights will be quantized. Check args.rt
All pretrained parameters loaded successfully
Files already downloaded and verified
Test before retraining:
Test: [0/79]	Time 28.401 (28.401)	Loss 2.5803 (2.5803)	Prec@1 41.406 (41.406)
Test: [60/79]	Time 10.594 (10.899)	Loss 2.3941 (2.7134)	Prec@1 46.875 (40.920)
 * Prec@1 40.980
Test Accuracy:40.98
Retraining:
current lr 1.00000e-04
Epoch: [0][0/391]	Time 17.515 (17.515)	Data 0.203 (0.203)	Loss 0.6217 (0.6217)	Prec@1 78.906 (78.906)
Epoch: [0][60/391]	Time 10.457 (10.643)	Data 0.000 (0.004)	Loss 0.4908 (0.5146)	Prec@1 83.594 (82.633)
Epoch: [0][120/391]	Time 10.442 (10.557)	Data 0.000 (0.002)	Loss 0.4467 (0.4880)	Prec@1 86.719 (83.555)
Epoch: [0][180/391]	Time 10.303 (10.509)	Data 0.000 (0.001)	Loss 0.4903 (0.4703)	Prec@1 87.500 (84.017)
Epoch: [0][240/391]	Time 10.351 (10.494)	Data 0.000 (0.001)	Loss 0.4853 (0.4622)	Prec@1 81.250 (84.177)
Epoch: [0][300/391]	Time 10.444 (10.483)	Data 0.000 (0.001)	Loss 0.5391 (0.4562)	Prec@1 82.031 (84.414)
Epoch: [0][360/391]	Time 10.410 (10.473)	Data 0.000 (0.001)	Loss 0.3254 (0.4502)	Prec@1 87.500 (84.607)
Test: [0/79]	Time 10.762 (10.762)	Loss 0.2746 (0.2746)	Prec@1 90.625 (90.625)
Test: [60/79]	Time 10.539 (10.507)	Loss 0.3627 (0.3569)	Prec@1 89.062 (88.934)
 * Prec@1 89.020
Best Accuracy:89.02
Saving the trained model as: Nov5_retrain_1.th
current lr 1.00000e-04
Epoch: [1][0/391]	Time 10.904 (10.904)	Data 0.258 (0.258)	Loss 0.3972 (0.3972)	Prec@1 83.594 (83.594)
Epoch: [1][60/391]	Time 10.509 (10.596)	Data 0.000 (0.004)	Loss 0.4310 (0.4180)	Prec@1 85.156 (85.182)
Epoch: [1][120/391]	Time 10.601 (10.563)	Data 0.000 (0.002)	Loss 0.2541 (0.4037)	Prec@1 89.844 (85.673)
Epoch: [1][180/391]	Time 10.580 (10.555)	Data 0.000 (0.002)	Loss 0.2792 (0.4066)	Prec@1 90.625 (85.687)
Epoch: [1][240/391]	Time 10.392 (10.541)	Data 0.000 (0.001)	Loss 0.4120 (0.4078)	Prec@1 89.062 (85.714)
Epoch: [1][300/391]	Time 10.509 (10.519)	Data 0.000 (0.001)	Loss 0.3400 (0.4100)	Prec@1 87.500 (85.657)
Epoch: [1][360/391]	Time 10.410 (10.507)	Data 0.000 (0.001)	Loss 0.4922 (0.4105)	Prec@1 79.688 (85.663)
Test: [0/79]	Time 10.398 (10.398)	Loss 0.2138 (0.2138)	Prec@1 93.750 (93.750)
Test: [60/79]	Time 10.232 (10.242)	Loss 0.3258 (0.3541)	Prec@1 88.281 (89.203)
 * Prec@1 89.190
Best Accuracy:89.19
Saving the trained model as: Nov5_retrain_1.th
current lr 1.00000e-04
Epoch: [2][0/391]	Time 10.560 (10.560)	Data 0.189 (0.189)	Loss 0.3270 (0.3270)	Prec@1 89.844 (89.844)
Epoch: [2][60/391]	Time 10.619 (10.452)	Data 0.000 (0.003)	Loss 0.4998 (0.4080)	Prec@1 83.594 (85.912)
Epoch: [2][120/391]	Time 10.464 (10.438)	Data 0.000 (0.002)	Loss 0.3340 (0.4024)	Prec@1 87.500 (85.912)
Epoch: [2][180/391]	Time 10.345 (10.435)	Data 0.000 (0.001)	Loss 0.3017 (0.4027)	Prec@1 90.625 (85.942)
Epoch: [2][240/391]	Time 10.382 (10.435)	Data 0.000 (0.001)	Loss 0.2156 (0.4037)	Prec@1 92.969 (85.938)
Epoch: [2][300/391]	Time 10.567 (10.435)	Data 0.000 (0.001)	Loss 0.4081 (0.4056)	Prec@1 84.375 (85.878)
Epoch: [2][360/391]	Time 10.379 (10.425)	Data 0.000 (0.001)	Loss 0.5273 (0.4049)	Prec@1 82.812 (85.940)
Test: [0/79]	Time 10.606 (10.606)	Loss 0.2731 (0.2731)	Prec@1 92.969 (92.969)
Test: [60/79]	Time 10.402 (10.423)	Loss 0.2844 (0.3564)	Prec@1 91.406 (88.858)
 * Prec@1 88.970
Best Accuracy:89.19
Saving the trained model as: Nov5_retrain_1.th
current lr 1.00000e-04
Epoch: [3][0/391]	Time 10.675 (10.675)	Data 0.244 (0.244)	Loss 0.2136 (0.2136)	Prec@1 91.406 (91.406)
Epoch: [3][60/391]	Time 10.543 (10.441)	Data 0.000 (0.004)	Loss 0.3526 (0.3877)	Prec@1 88.281 (86.411)
Epoch: [3][120/391]	Time 10.369 (10.436)	Data 0.000 (0.002)	Loss 0.6040 (0.3952)	Prec@1 78.125 (86.086)
Epoch: [3][180/391]	Time 10.281 (10.430)	Data 0.000 (0.002)	Loss 0.4438 (0.4033)	Prec@1 83.594 (85.873)
Epoch: [3][240/391]	Time 10.454 (10.427)	Data 0.000 (0.001)	Loss 0.4656 (0.4079)	Prec@1 87.500 (85.850)
Epoch: [3][300/391]	Time 10.442 (10.423)	Data 0.000 (0.001)	Loss 0.3960 (0.4106)	Prec@1 87.500 (85.766)
Epoch: [3][360/391]	Time 10.413 (10.419)	Data 0.000 (0.001)	Loss 0.4384 (0.4114)	Prec@1 84.375 (85.719)
Test: [0/79]	Time 10.823 (10.823)	Loss 0.2793 (0.2793)	Prec@1 91.406 (91.406)
Test: [60/79]	Time 10.363 (10.432)	Loss 0.2741 (0.3650)	Prec@1 90.625 (88.934)
 * Prec@1 89.050
Best Accuracy:89.19
Saving the trained model as: Nov5_retrain_1.th
current lr 1.00000e-04
Epoch: [4][0/391]	Time 10.652 (10.652)	Data 0.205 (0.205)	Loss 0.2432 (0.2432)	Prec@1 91.406 (91.406)
Epoch: [4][60/391]	Time 10.391 (10.425)	Data 0.000 (0.004)	Loss 0.4199 (0.4199)	Prec@1 82.812 (85.246)
Epoch: [4][120/391]	Time 10.397 (10.427)	Data 0.000 (0.002)	Loss 0.3781 (0.4164)	Prec@1 85.156 (85.311)
Epoch: [4][180/391]	Time 10.362 (10.429)	Data 0.000 (0.001)	Loss 0.4524 (0.4214)	Prec@1 85.938 (85.355)
Epoch: [4][240/391]	Time 10.493 (10.429)	Data 0.000 (0.001)	Loss 0.4396 (0.4156)	Prec@1 83.594 (85.536)
Epoch: [4][300/391]	Time 10.473 (10.433)	Data 0.000 (0.001)	Loss 0.3402 (0.4157)	Prec@1 86.719 (85.533)
Epoch: [4][360/391]	Time 10.479 (10.429)	Data 0.000 (0.001)	Loss 0.3587 (0.4177)	Prec@1 85.938 (85.470)
Test: [0/79]	Time 10.611 (10.611)	Loss 0.2435 (0.2435)	Prec@1 91.406 (91.406)
Test: [60/79]	Time 10.507 (10.421)	Loss 0.3259 (0.3735)	Prec@1 90.625 (88.717)
 * Prec@1 88.750
Best Accuracy:89.19
Saving the trained model as: Nov5_retrain_1.th
current lr 1.00000e-04
Epoch: [5][0/391]	Time 10.642 (10.642)	Data 0.191 (0.191)	Loss 0.4767 (0.4767)	Prec@1 86.719 (86.719)
Epoch: [5][60/391]	Time 10.321 (10.438)	Data 0.000 (0.003)	Loss 0.4257 (0.4091)	Prec@1 85.156 (85.950)
Epoch: [5][120/391]	Time 10.384 (10.445)	Data 0.000 (0.002)	Loss 0.3880 (0.4154)	Prec@1 82.812 (85.705)
Epoch: [5][180/391]	Time 10.333 (10.442)	Data 0.000 (0.001)	Loss 0.4706 (0.4169)	Prec@1 85.156 (85.674)
Epoch: [5][240/391]	Time 10.459 (10.437)	Data 0.000 (0.001)	Loss 0.3310 (0.4195)	Prec@1 89.062 (85.571)
Epoch: [5][300/391]	Time 10.372 (10.440)	Data 0.000 (0.001)	Loss 0.4226 (0.4171)	Prec@1 85.156 (85.644)
Epoch: [5][360/391]	Time 10.487 (10.438)	Data 0.000 (0.001)	Loss 0.3662 (0.4189)	Prec@1 85.938 (85.539)
Test: [0/79]	Time 10.545 (10.545)	Loss 0.1950 (0.1950)	Prec@1 95.312 (95.312)
Test: [60/79]	Time 10.294 (10.267)	Loss 0.3141 (0.3821)	Prec@1 91.406 (88.358)
 * Prec@1 88.420
Best Accuracy:89.19
Saving the trained model as: Nov5_retrain_1.th
current lr 1.00000e-05
Epoch: [6][0/391]	Time 10.643 (10.643)	Data 0.197 (0.197)	Loss 0.4191 (0.4191)	Prec@1 83.594 (83.594)
Epoch: [6][60/391]	Time 10.215 (10.276)	Data 0.000 (0.003)	Loss 0.3335 (0.4159)	Prec@1 86.719 (85.143)
Epoch: [6][120/391]	Time 10.255 (10.278)	Data 0.000 (0.002)	Loss 0.3961 (0.4216)	Prec@1 85.156 (85.130)
Epoch: [6][180/391]	Time 10.251 (10.272)	Data 0.000 (0.001)	Loss 0.3275 (0.4242)	Prec@1 88.281 (85.130)
Epoch: [6][240/391]	Time 10.283 (10.278)	Data 0.000 (0.001)	Loss 0.4374 (0.4259)	Prec@1 83.594 (85.150)
Epoch: [6][300/391]	Time 10.381 (10.283)	Data 0.000 (0.001)	Loss 0.3323 (0.4220)	Prec@1 85.938 (85.260)
Epoch: [6][360/391]	Time 10.210 (10.286)	Data 0.000 (0.001)	Loss 0.5377 (0.4191)	Prec@1 82.812 (85.431)
Test: [0/79]	Time 10.408 (10.408)	Loss 0.2778 (0.2778)	Prec@1 92.188 (92.188)
Test: [60/79]	Time 10.404 (10.267)	Loss 0.2937 (0.3737)	Prec@1 90.625 (88.601)
 * Prec@1 88.640
Best Accuracy:89.19
Saving the trained model as: Nov5_retrain_1.th
current lr 1.00000e-05
Epoch: [7][0/391]	Time 10.759 (10.759)	Data 0.258 (0.258)	Loss 0.3478 (0.3478)	Prec@1 88.281 (88.281)
Epoch: [7][60/391]	Time 10.389 (10.451)	Data 0.000 (0.004)	Loss 0.2901 (0.4216)	Prec@1 88.281 (85.156)
Epoch: [7][120/391]	Time 10.524 (10.446)	Data 0.000 (0.002)	Loss 0.4996 (0.4164)	Prec@1 85.156 (85.615)
Epoch: [7][180/391]	Time 10.475 (10.434)	Data 0.000 (0.002)	Loss 0.4416 (0.4168)	Prec@1 82.812 (85.484)
Epoch: [7][240/391]	Time 10.536 (10.431)	Data 0.000 (0.001)	Loss 0.4231 (0.4173)	Prec@1 79.688 (85.477)
Epoch: [7][300/391]	Time 10.248 (10.401)	Data 0.000 (0.001)	Loss 0.3871 (0.4153)	Prec@1 88.281 (85.569)
Epoch: [7][360/391]	Time 10.303 (10.378)	Data 0.000 (0.001)	Loss 0.4087 (0.4147)	Prec@1 85.938 (85.624)
Test: [0/79]	Time 10.535 (10.535)	Loss 0.2789 (0.2789)	Prec@1 89.844 (89.844)
Test: [60/79]	Time 10.234 (10.290)	Loss 0.3067 (0.3762)	Prec@1 89.844 (88.448)
 * Prec@1 88.500
Best Accuracy:89.19
Saving the trained model as: Nov5_retrain_1.th
current lr 1.00000e-05
Epoch: [8][0/391]	Time 10.437 (10.437)	Data 0.220 (0.220)	Loss 0.3804 (0.3804)	Prec@1 83.594 (83.594)
Epoch: [8][60/391]	Time 10.509 (10.481)	Data 0.000 (0.004)	Loss 0.2995 (0.4069)	Prec@1 89.062 (85.579)
Epoch: [8][120/391]	Time 10.464 (10.470)	Data 0.000 (0.002)	Loss 0.3163 (0.4165)	Prec@1 91.406 (85.305)
Epoch: [8][180/391]	Time 10.501 (10.472)	Data 0.000 (0.001)	Loss 0.3840 (0.4158)	Prec@1 88.281 (85.389)
Epoch: [8][240/391]	Time 10.584 (10.467)	Data 0.000 (0.001)	Loss 0.4229 (0.4138)	Prec@1 84.375 (85.464)
Epoch: [8][300/391]	Time 10.408 (10.465)	Data 0.000 (0.001)	Loss 0.4043 (0.4134)	Prec@1 87.500 (85.509)
Epoch: [8][360/391]	Time 10.559 (10.470)	Data 0.000 (0.001)	Loss 0.4361 (0.4193)	Prec@1 84.375 (85.295)
Test: [0/79]	Time 10.433 (10.433)	Loss 0.2710 (0.2710)	Prec@1 87.500 (87.500)
Test: [60/79]	Time 10.167 (10.253)	Loss 0.3052 (0.3794)	Prec@1 90.625 (88.358)
 * Prec@1 88.470
Best Accuracy:89.19
Saving the trained model as: Nov5_retrain_1.th
current lr 1.00000e-05
Epoch: [9][0/391]	Time 10.517 (10.517)	Data 0.281 (0.281)	Loss 0.3641 (0.3641)	Prec@1 86.719 (86.719)
Epoch: [9][60/391]	Time 10.365 (10.319)	Data 0.000 (0.005)	Loss 0.3365 (0.4186)	Prec@1 87.500 (85.169)
Epoch: [9][120/391]	Time 10.324 (10.301)	Data 0.000 (0.003)	Loss 0.5119 (0.4196)	Prec@1 83.594 (85.292)
Epoch: [9][180/391]	Time 10.201 (10.292)	Data 0.000 (0.002)	Loss 0.5250 (0.4166)	Prec@1 79.688 (85.463)
Epoch: [9][240/391]	Time 10.416 (10.291)	Data 0.000 (0.001)	Loss 0.4020 (0.4203)	Prec@1 83.594 (85.412)
Epoch: [9][300/391]	Time 10.359 (10.290)	Data 0.000 (0.001)	Loss 0.4396 (0.4211)	Prec@1 85.938 (85.374)
Epoch: [9][360/391]	Time 10.443 (10.289)	Data 0.000 (0.001)	Loss 0.5259 (0.4203)	Prec@1 81.250 (85.358)
Test: [0/79]	Time 10.711 (10.711)	Loss 0.2358 (0.2358)	Prec@1 92.969 (92.969)
Test: [60/79]	Time 10.409 (10.443)	Loss 0.2769 (0.3723)	Prec@1 91.406 (88.448)
 * Prec@1 88.460
Best Accuracy:89.19
Saving the trained model as: Nov5_retrain_1.th
current lr 1.00000e-06
Epoch: [10][0/391]	Time 10.726 (10.726)	Data 0.266 (0.266)	Loss 0.4029 (0.4029)	Prec@1 85.156 (85.156)
Epoch: [10][60/391]	Time 10.399 (10.439)	Data 0.000 (0.005)	Loss 0.4200 (0.4193)	Prec@1 88.281 (85.297)
Epoch: [10][120/391]	Time 10.593 (10.443)	Data 0.000 (0.002)	Loss 0.3904 (0.4197)	Prec@1 86.719 (85.434)
Epoch: [10][180/391]	Time 10.393 (10.447)	Data 0.000 (0.002)	Loss 0.4418 (0.4213)	Prec@1 82.812 (85.281)
Epoch: [10][240/391]	Time 10.400 (10.449)	Data 0.000 (0.001)	Loss 0.3832 (0.4179)	Prec@1 87.500 (85.429)
Epoch: [10][300/391]	Time 10.532 (10.449)	Data 0.000 (0.001)	Loss 0.3898 (0.4166)	Prec@1 87.500 (85.434)
Epoch: [10][360/391]	Time 10.456 (10.452)	Data 0.000 (0.001)	Loss 0.4459 (0.4156)	Prec@1 83.594 (85.440)
Test: [0/79]	Time 10.474 (10.474)	Loss 0.2450 (0.2450)	Prec@1 92.188 (92.188)
Test: [60/79]	Time 10.361 (10.263)	Loss 0.3109 (0.3710)	Prec@1 92.188 (88.397)
 * Prec@1 88.520
Best Accuracy:89.19
Saving the trained model as: Nov5_retrain_1.th
current lr 1.00000e-06
Epoch: [11][0/391]	Time 10.623 (10.623)	Data 0.227 (0.227)	Loss 0.4256 (0.4256)	Prec@1 85.938 (85.938)
Epoch: [11][60/391]	Time 10.358 (10.293)	Data 0.000 (0.004)	Loss 0.3514 (0.4404)	Prec@1 89.844 (85.067)
Epoch: [11][120/391]	Time 10.329 (10.288)	Data 0.000 (0.002)	Loss 0.4780 (0.4397)	Prec@1 82.031 (84.911)
Epoch: [11][180/391]	Time 10.355 (10.281)	Data 0.000 (0.002)	Loss 0.3644 (0.4273)	Prec@1 86.719 (85.178)
Epoch: [11][240/391]	Time 10.230 (10.278)	Data 0.000 (0.001)	Loss 0.3947 (0.4272)	Prec@1 85.156 (85.195)
Epoch: [11][300/391]	Time 10.365 (10.277)	Data 0.000 (0.001)	Loss 0.3618 (0.4246)	Prec@1 87.500 (85.281)
Epoch: [11][360/391]	Time 10.207 (10.276)	Data 0.000 (0.001)	Loss 0.3915 (0.4255)	Prec@1 86.719 (85.208)
Test: [0/79]	Time 10.625 (10.625)	Loss 0.2467 (0.2467)	Prec@1 92.969 (92.969)
Test: [60/79]	Time 10.382 (10.287)	Loss 0.3235 (0.3740)	Prec@1 89.844 (88.486)
 * Prec@1 88.600
Best Accuracy:89.19
Saving the trained model as: Nov5_retrain_1.th
current lr 1.00000e-06
Epoch: [12][0/391]	Time 10.586 (10.586)	Data 0.241 (0.241)	Loss 0.5188 (0.5188)	Prec@1 81.250 (81.250)
Epoch: [12][60/391]	Time 10.174 (10.269)	Data 0.000 (0.004)	Loss 0.4499 (0.4223)	Prec@1 83.594 (85.707)
Epoch: [12][120/391]	Time 10.307 (10.271)	Data 0.000 (0.002)	Loss 0.2632 (0.4164)	Prec@1 91.406 (85.724)
Epoch: [12][180/391]	Time 10.167 (10.273)	Data 0.000 (0.002)	Loss 0.5291 (0.4200)	Prec@1 80.469 (85.506)
Epoch: [12][240/391]	Time 10.258 (10.278)	Data 0.000 (0.001)	Loss 0.5917 (0.4243)	Prec@1 78.906 (85.338)
Epoch: [12][300/391]	Time 10.349 (10.281)	Data 0.000 (0.001)	Loss 0.3905 (0.4250)	Prec@1 85.938 (85.252)
Epoch: [12][360/391]	Time 10.414 (10.284)	Data 0.000 (0.001)	Loss 0.5419 (0.4255)	Prec@1 81.250 (85.202)
Test: [0/79]	Time 10.506 (10.506)	Loss 0.2474 (0.2474)	Prec@1 94.531 (94.531)
Test: [60/79]	Time 10.171 (10.282)	Loss 0.3045 (0.3761)	Prec@1 89.062 (88.448)
 * Prec@1 88.520
Best Accuracy:89.19
Saving the trained model as: Nov5_retrain_1.th
current lr 1.00000e-06
Epoch: [13][0/391]	Time 10.608 (10.608)	Data 0.239 (0.239)	Loss 0.3210 (0.3210)	Prec@1 90.625 (90.625)
Epoch: [13][60/391]	Time 10.265 (10.293)	Data 0.000 (0.004)	Loss 0.3809 (0.4260)	Prec@1 87.500 (85.489)
Epoch: [13][120/391]	Time 10.379 (10.294)	Data 0.000 (0.002)	Loss 0.4068 (0.4322)	Prec@1 82.031 (85.079)
Epoch: [13][180/391]	Time 10.348 (10.293)	Data 0.000 (0.002)	Loss 0.4390 (0.4295)	Prec@1 85.938 (85.096)
Epoch: [13][240/391]	Time 10.342 (10.289)	Data 0.000 (0.001)	Loss 0.3082 (0.4290)	Prec@1 88.281 (85.182)
Epoch: [13][300/391]	Time 10.263 (10.284)	Data 0.000 (0.001)	Loss 0.4802 (0.4250)	Prec@1 84.375 (85.312)
Epoch: [13][360/391]	Time 10.199 (10.282)	Data 0.000 (0.001)	Loss 0.4145 (0.4225)	Prec@1 84.375 (85.340)
Test: [0/79]	Time 10.604 (10.604)	Loss 0.2485 (0.2485)	Prec@1 92.969 (92.969)
Test: [60/79]	Time 10.256 (10.262)	Loss 0.2754 (0.3771)	Prec@1 92.188 (88.461)
 * Prec@1 88.420
Best Accuracy:89.19
Saving the trained model as: Nov5_retrain_1.th
current lr 1.00000e-07
Epoch: [14][0/391]	Time 10.797 (10.797)	Data 0.206 (0.206)	Loss 0.3442 (0.3442)	Prec@1 86.719 (86.719)
Epoch: [14][60/391]	Time 10.391 (10.488)	Data 0.000 (0.004)	Loss 0.5193 (0.4219)	Prec@1 80.469 (85.336)
Epoch: [14][120/391]	Time 10.501 (10.465)	Data 0.000 (0.002)	Loss 0.4290 (0.4241)	Prec@1 85.156 (85.292)
Epoch: [14][180/391]	Time 10.486 (10.465)	Data 0.000 (0.001)	Loss 0.5039 (0.4241)	Prec@1 82.812 (85.113)
Epoch: [14][240/391]	Time 10.450 (10.457)	Data 0.000 (0.001)	Loss 0.4550 (0.4248)	Prec@1 81.250 (85.091)
Epoch: [14][300/391]	Time 10.434 (10.459)	Data 0.000 (0.001)	Loss 0.5619 (0.4261)	Prec@1 84.375 (85.086)
Epoch: [14][360/391]	Time 10.445 (10.461)	Data 0.000 (0.001)	Loss 0.4458 (0.4220)	Prec@1 85.938 (85.217)
Test: [0/79]	Time 10.854 (10.854)	Loss 0.2542 (0.2542)	Prec@1 92.188 (92.188)
Test: [60/79]	Time 10.440 (10.469)	Loss 0.2925 (0.3733)	Prec@1 89.844 (88.435)
 * Prec@1 88.430
Best Accuracy:89.19
Saving the trained model as: Nov5_retrain_1.th
current lr 1.00000e-07
Epoch: [15][0/391]	Time 10.764 (10.764)	Data 0.262 (0.262)	Loss 0.4896 (0.4896)	Prec@1 81.250 (81.250)
Epoch: [15][60/391]	Time 10.536 (10.456)	Data 0.000 (0.005)	Loss 0.5809 (0.4177)	Prec@1 82.812 (85.694)
Epoch: [15][120/391]	Time 10.407 (10.455)	Data 0.000 (0.002)	Loss 0.4297 (0.4244)	Prec@1 82.812 (85.111)
Epoch: [15][180/391]	Time 10.416 (10.460)	Data 0.000 (0.002)	Loss 0.3199 (0.4214)	Prec@1 86.719 (85.057)
Epoch: [15][240/391]	Time 10.523 (10.466)	Data 0.000 (0.001)	Loss 0.5126 (0.4186)	Prec@1 84.375 (85.215)
Epoch: [15][300/391]	Time 10.478 (10.469)	Data 0.000 (0.001)	Loss 0.3652 (0.4175)	Prec@1 88.281 (85.268)
Epoch: [15][360/391]	Time 10.375 (10.445)	Data 0.000 (0.001)	Loss 0.4424 (0.4171)	Prec@1 82.812 (85.334)
Test: [0/79]	Time 10.619 (10.619)	Loss 0.2669 (0.2669)	Prec@1 91.406 (91.406)
Test: [60/79]	Time 10.425 (10.431)	Loss 0.3137 (0.3825)	Prec@1 89.844 (88.537)
 * Prec@1 88.560
Best Accuracy:89.19
Saving the trained model as: Nov5_retrain_1.th
current lr 1.00000e-07
Epoch: [16][0/391]	Time 10.827 (10.827)	Data 0.280 (0.280)	Loss 0.5213 (0.5213)	Prec@1 80.469 (80.469)
Epoch: [16][60/391]	Time 10.483 (10.493)	Data 0.000 (0.005)	Loss 0.3488 (0.4157)	Prec@1 85.938 (85.003)
Epoch: [16][120/391]	Time 10.570 (10.490)	Data 0.000 (0.003)	Loss 0.3886 (0.4189)	Prec@1 89.844 (85.027)
Epoch: [16][180/391]	Time 10.430 (10.488)	Data 0.000 (0.002)	Loss 0.5402 (0.4181)	Prec@1 82.031 (85.186)
Epoch: [16][240/391]	Time 10.236 (10.446)	Data 0.000 (0.001)	Loss 0.4349 (0.4203)	Prec@1 86.719 (85.159)
Epoch: [16][300/391]	Time 10.317 (10.418)	Data 0.000 (0.001)	Loss 0.5532 (0.4206)	Prec@1 82.812 (85.187)
Epoch: [16][360/391]	Time 10.373 (10.401)	Data 0.000 (0.001)	Loss 0.6214 (0.4244)	Prec@1 76.562 (85.113)
Test: [0/79]	Time 10.506 (10.506)	Loss 0.2231 (0.2231)	Prec@1 92.969 (92.969)
Test: [60/79]	Time 10.473 (10.387)	Loss 0.2963 (0.3779)	Prec@1 91.406 (88.217)
 * Prec@1 88.230
Best Accuracy:89.19
Saving the trained model as: Nov5_retrain_1.th
current lr 1.00000e-07
Epoch: [17][0/391]	Time 10.674 (10.674)	Data 0.196 (0.196)	Loss 0.4532 (0.4532)	Prec@1 86.719 (86.719)
Epoch: [17][60/391]	Time 10.519 (10.428)	Data 0.000 (0.003)	Loss 0.4681 (0.4195)	Prec@1 83.594 (85.451)
Epoch: [17][120/391]	Time 10.467 (10.427)	Data 0.000 (0.002)	Loss 0.2625 (0.4233)	Prec@1 89.062 (85.182)
Epoch: [17][180/391]	Time 10.348 (10.432)	Data 0.000 (0.001)	Loss 0.3325 (0.4219)	Prec@1 85.938 (85.346)
Epoch: [17][240/391]	Time 10.421 (10.434)	Data 0.000 (0.001)	Loss 0.2621 (0.4230)	Prec@1 91.406 (85.312)
Epoch: [17][300/391]	Time 10.588 (10.435)	Data 0.000 (0.001)	Loss 0.6702 (0.4284)	Prec@1 76.562 (85.123)
Epoch: [17][360/391]	Time 10.414 (10.437)	Data 0.000 (0.001)	Loss 0.4498 (0.4259)	Prec@1 85.938 (85.241)
Test: [0/79]	Time 10.737 (10.737)	Loss 0.2554 (0.2554)	Prec@1 91.406 (91.406)
Test: [60/79]	Time 10.390 (10.456)	Loss 0.3070 (0.3745)	Prec@1 89.062 (88.192)
 * Prec@1 88.250
Best Accuracy:89.19
Saving the trained model as: Nov5_retrain_1.th
current lr 1.00000e-08
Epoch: [18][0/391]	Time 10.749 (10.749)	Data 0.284 (0.284)	Loss 0.4588 (0.4588)	Prec@1 84.375 (84.375)
Epoch: [18][60/391]	Time 10.566 (10.446)	Data 0.000 (0.005)	Loss 0.4610 (0.4216)	Prec@1 82.812 (85.400)
Epoch: [18][120/391]	Time 10.444 (10.447)	Data 0.000 (0.003)	Loss 0.4471 (0.4231)	Prec@1 87.500 (85.124)
Epoch: [18][180/391]	Time 10.479 (10.441)	Data 0.000 (0.002)	Loss 0.3638 (0.4227)	Prec@1 88.281 (85.225)
Epoch: [18][240/391]	Time 10.438 (10.441)	Data 0.000 (0.001)	Loss 0.4959 (0.4193)	Prec@1 79.688 (85.373)
Epoch: [18][300/391]	Time 10.365 (10.438)	Data 0.000 (0.001)	Loss 0.3525 (0.4232)	Prec@1 87.500 (85.244)
Epoch: [18][360/391]	Time 10.466 (10.439)	Data 0.000 (0.001)	Loss 0.3300 (0.4227)	Prec@1 89.062 (85.256)
Test: [0/79]	Time 10.723 (10.723)	Loss 0.2350 (0.2350)	Prec@1 92.188 (92.188)
Test: [60/79]	Time 10.424 (10.448)	Loss 0.3208 (0.3745)	Prec@1 90.625 (88.486)
 * Prec@1 88.510
Best Accuracy:89.19
Saving the trained model as: Nov5_retrain_1.th
current lr 1.00000e-08
Epoch: [19][0/391]	Time 10.856 (10.856)	Data 0.237 (0.237)	Loss 0.3840 (0.3840)	Prec@1 90.625 (90.625)
Epoch: [19][60/391]	Time 10.357 (10.457)	Data 0.000 (0.004)	Loss 0.3532 (0.4337)	Prec@1 89.062 (85.015)
Epoch: [19][120/391]	Time 10.441 (10.448)	Data 0.000 (0.002)	Loss 0.4966 (0.4212)	Prec@1 83.594 (85.427)
Epoch: [19][180/391]	Time 10.427 (10.445)	Data 0.000 (0.002)	Loss 0.3410 (0.4187)	Prec@1 90.625 (85.540)
Epoch: [19][240/391]	Time 10.466 (10.443)	Data 0.000 (0.001)	Loss 0.4600 (0.4235)	Prec@1 85.156 (85.325)
Epoch: [19][300/391]	Time 10.419 (10.445)	Data 0.000 (0.001)	Loss 0.4662 (0.4216)	Prec@1 82.031 (85.351)
Epoch: [19][360/391]	Time 10.380 (10.447)	Data 0.000 (0.001)	Loss 0.4813 (0.4226)	Prec@1 85.156 (85.379)
Test: [0/79]	Time 10.417 (10.417)	Loss 0.2557 (0.2557)	Prec@1 93.750 (93.750)
Test: [60/79]	Time 10.215 (10.260)	Loss 0.3090 (0.3649)	Prec@1 92.188 (88.870)
 * Prec@1 88.780
Best Accuracy:89.19
Saving the trained model as: Nov5_retrain_1.th
