MVM Config:--------------
Bit Stream:  2
Bit Slice:  1
ADC Bit:  4
Xbar size:  128 128
Real ADC Characteristics:  True ideal
Xbar Weight Std Gamma 0.0
WARNING: crossbar sizes with different row annd column dimension not supported.
MVM Config:--------------
Bit Stream:  2
Bit Slice:  1
ADC Bit:  4
Xbar size:  128 128
Real ADC Characteristics:  True ideal
Xbar Weight Std Gamma 0.0
WARNING: crossbar sizes with different row annd column dimension not supported.
Available models:
resnet20_adc resnet110 resnet1202 resnet20 resnet32 resnet44 resnet56 resnet8 resnet20_mvm
Loading pretrained model from  saved_models/resnet20_qwa_nobias_1_12Aug.th
Pretrained model training accuracy: 93.05
-Starting retraining on mvm model, fp model weights will be quantized. Check args.rt
All pretrained parameters loaded successfully
Files already downloaded and verified
Test before retraining:
Traceback (most recent call last):
  File "/home/dynamo/a/debnathm/func_modelling/puma_functional_model/MyWorks/retrainer_mvm.py", line 501, in <module>
    main()
  File "/home/dynamo/a/debnathm/func_modelling/puma_functional_model/MyWorks/retrainer_mvm.py", line 319, in main
    test_result = validate(val_loader, model, criterion)
  File "/home/dynamo/a/debnathm/func_modelling/puma_functional_model/MyWorks/retrainer_mvm.py", line 427, in validate
    output = model(input_var)
  File "/home/dynamo/a/debnathm/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/dynamo/a/debnathm/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 171, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/dynamo/a/debnathm/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 181, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/dynamo/a/debnathm/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py", line 89, in parallel_apply
    output.reraise()
  File "/home/dynamo/a/debnathm/anaconda3/lib/python3.10/site-packages/torch/_utils.py", line 644, in reraise
    raise exception
torch.cuda.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/dynamo/a/debnathm/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py", line 64, in _worker
    output = module(*input, **kwargs)
  File "/home/dynamo/a/debnathm/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/dynamo/a/debnathm/func_modelling/puma_functional_model/models/resnet_mvm.py", line 131, in forward
    out = self.layer2(out)
  File "/home/dynamo/a/debnathm/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/dynamo/a/debnathm/anaconda3/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/dynamo/a/debnathm/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/dynamo/a/debnathm/func_modelling/puma_functional_model/models/resnet_mvm.py", line 88, in forward
    out = self.conv1(x)
  File "/home/dynamo/a/debnathm/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/dynamo/a/debnathm/func_modelling/puma_functional_model/src/pytorch_mvm_class_v3.py", line 332, in forward
    return Conv2d_mvm_function.apply(input, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups,
  File "/home/dynamo/a/debnathm/anaconda3/lib/python3.10/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/dynamo/a/debnathm/func_modelling/puma_functional_model/src/pytorch_mvm_class_v3.py", line 189, in forward
    xbars_out = mvm_tensor(zero_mvmtensor, shift_add_bit_stream, shift_add_bit_slice, output_reg, flatten_binary_input_xbar, flatten_input_sign_xbar,
  File "/home/dynamo/a/debnathm/func_modelling/puma_functional_model/src/mvm_v3.py", line 145, in mvm_tensor
    output_analog = torch.mul(xbars, input_stream)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 44.35 GiB total capacity; 79.34 MiB already allocated; 616.19 MiB free; 300.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

