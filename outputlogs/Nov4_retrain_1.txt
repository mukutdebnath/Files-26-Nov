MVM Config:--------------
Bit Stream:  1
Bit Slice:  2
ADC Bit:  4
Xbar size:  128 128
Real ADC Characteristics:  True ideal
Xbar Weight Std Gamma 0.0
WARNING: crossbar sizes with different row annd column dimension not supported.
MVM Config:--------------
Bit Stream:  1
Bit Slice:  2
ADC Bit:  4
Xbar size:  128 128
Real ADC Characteristics:  True ideal
Xbar Weight Std Gamma 0.0
WARNING: crossbar sizes with different row annd column dimension not supported.
Available models:
resnet20_adc resnet110 resnet1202 resnet20 resnet32 resnet44 resnet56 resnet8 resnet20_mvm
Loading pretrained model from  saved_models/resnet20_qwa_nobias_1_12Aug.th
Pretrained model training accuracy: 93.05
-Starting retraining on mvm model, fp model weights will be quantized. Check args.rt
All pretrained parameters loaded successfully
Files already downloaded and verified
Test before retraining:
Test: [0/79]	Time 23.945 (23.945)	Loss 2.5803 (2.5803)	Prec@1 41.406 (41.406)
Test: [60/79]	Time 10.274 (10.749)	Loss 2.3941 (2.7134)	Prec@1 46.875 (40.920)
 * Prec@1 40.980
Test Accuracy:40.98
Retraining:
current lr 1.00000e-04
Epoch: [0][0/391]	Time 17.027 (17.027)	Data 0.170 (0.170)	Loss 0.6217 (0.6217)	Prec@1 78.906 (78.906)
Epoch: [0][60/391]	Time 10.210 (10.422)	Data 0.000 (0.003)	Loss 0.4908 (0.5146)	Prec@1 83.594 (82.633)
Epoch: [0][120/391]	Time 10.249 (10.342)	Data 0.000 (0.002)	Loss 0.4467 (0.4880)	Prec@1 86.719 (83.555)
Epoch: [0][180/391]	Time 10.436 (10.330)	Data 0.000 (0.001)	Loss 0.4903 (0.4703)	Prec@1 87.500 (84.017)
Epoch: [0][240/391]	Time 10.480 (10.347)	Data 0.000 (0.001)	Loss 0.4853 (0.4622)	Prec@1 81.250 (84.177)
Epoch: [0][300/391]	Time 10.403 (10.359)	Data 0.000 (0.001)	Loss 0.5391 (0.4562)	Prec@1 82.031 (84.414)
Epoch: [0][360/391]	Time 10.493 (10.370)	Data 0.000 (0.001)	Loss 0.3254 (0.4502)	Prec@1 87.500 (84.607)
Test: [0/79]	Time 10.536 (10.536)	Loss 0.2746 (0.2746)	Prec@1 90.625 (90.625)
Test: [60/79]	Time 10.384 (10.405)	Loss 0.3627 (0.3569)	Prec@1 89.062 (88.934)
 * Prec@1 89.020
Best Accuracy:89.02
Saving the trained model as: Nov4_retrain_1.th
current lr 1.00000e-04
Epoch: [1][0/391]	Time 10.614 (10.614)	Data 0.194 (0.194)	Loss 0.3972 (0.3972)	Prec@1 83.594 (83.594)
Epoch: [1][60/391]	Time 10.549 (10.498)	Data 0.000 (0.003)	Loss 0.4310 (0.4180)	Prec@1 85.156 (85.182)
Epoch: [1][120/391]	Time 10.359 (10.494)	Data 0.000 (0.002)	Loss 0.2541 (0.4037)	Prec@1 89.844 (85.673)
Epoch: [1][180/391]	Time 10.463 (10.494)	Data 0.000 (0.001)	Loss 0.2792 (0.4066)	Prec@1 90.625 (85.687)
Epoch: [1][240/391]	Time 10.445 (10.496)	Data 0.000 (0.001)	Loss 0.4120 (0.4078)	Prec@1 89.062 (85.714)
Epoch: [1][300/391]	Time 10.351 (10.486)	Data 0.000 (0.001)	Loss 0.3400 (0.4100)	Prec@1 87.500 (85.657)
Epoch: [1][360/391]	Time 10.397 (10.474)	Data 0.000 (0.001)	Loss 0.4922 (0.4105)	Prec@1 79.688 (85.663)
Test: [0/79]	Time 10.665 (10.665)	Loss 0.2138 (0.2138)	Prec@1 93.750 (93.750)
Test: [60/79]	Time 10.300 (10.371)	Loss 0.3258 (0.3541)	Prec@1 88.281 (89.203)
 * Prec@1 89.190
Best Accuracy:89.19
Saving the trained model as: Nov4_retrain_1.th
current lr 1.00000e-04
Epoch: [2][0/391]	Time 10.578 (10.578)	Data 0.175 (0.175)	Loss 0.3270 (0.3270)	Prec@1 89.844 (89.844)
Epoch: [2][60/391]	Time 10.444 (10.487)	Data 0.000 (0.003)	Loss 0.4998 (0.4080)	Prec@1 83.594 (85.912)
Epoch: [2][120/391]	Time 10.322 (10.487)	Data 0.000 (0.002)	Loss 0.3340 (0.4024)	Prec@1 87.500 (85.912)
Epoch: [2][180/391]	Time 10.351 (10.458)	Data 0.000 (0.001)	Loss 0.3017 (0.4027)	Prec@1 90.625 (85.942)
Epoch: [2][240/391]	Time 10.290 (10.428)	Data 0.000 (0.001)	Loss 0.2156 (0.4037)	Prec@1 92.969 (85.938)
Epoch: [2][300/391]	Time 10.343 (10.405)	Data 0.000 (0.001)	Loss 0.4081 (0.4056)	Prec@1 84.375 (85.878)
Epoch: [2][360/391]	Time 10.194 (10.388)	Data 0.000 (0.001)	Loss 0.5273 (0.4049)	Prec@1 82.812 (85.940)
Test: [0/79]	Time 10.823 (10.823)	Loss 0.2731 (0.2731)	Prec@1 92.969 (92.969)
Test: [60/79]	Time 10.463 (10.466)	Loss 0.2844 (0.3564)	Prec@1 91.406 (88.858)
 * Prec@1 88.970
Best Accuracy:89.19
Saving the trained model as: Nov4_retrain_1.th
current lr 1.00000e-04
Epoch: [3][0/391]	Time 10.782 (10.782)	Data 0.244 (0.244)	Loss 0.2136 (0.2136)	Prec@1 91.406 (91.406)
Epoch: [3][60/391]	Time 10.363 (10.455)	Data 0.000 (0.004)	Loss 0.3526 (0.3877)	Prec@1 88.281 (86.411)
Epoch: [3][120/391]	Time 10.340 (10.432)	Data 0.000 (0.002)	Loss 0.6040 (0.3952)	Prec@1 78.125 (86.086)
Epoch: [3][180/391]	Time 10.375 (10.427)	Data 0.000 (0.002)	Loss 0.4438 (0.4033)	Prec@1 83.594 (85.873)
Epoch: [3][240/391]	Time 10.444 (10.416)	Data 0.000 (0.001)	Loss 0.4656 (0.4079)	Prec@1 87.500 (85.850)
Epoch: [3][300/391]	Time 10.582 (10.427)	Data 0.000 (0.001)	Loss 0.3960 (0.4106)	Prec@1 87.500 (85.766)
Epoch: [3][360/391]	Time 10.467 (10.440)	Data 0.000 (0.001)	Loss 0.4384 (0.4114)	Prec@1 84.375 (85.719)
Test: [0/79]	Time 10.617 (10.617)	Loss 0.2793 (0.2793)	Prec@1 91.406 (91.406)
Test: [60/79]	Time 10.464 (10.413)	Loss 0.2741 (0.3650)	Prec@1 90.625 (88.934)
 * Prec@1 89.050
Best Accuracy:89.19
Saving the trained model as: Nov4_retrain_1.th
current lr 1.00000e-04
Epoch: [4][0/391]	Time 10.681 (10.681)	Data 0.207 (0.207)	Loss 0.2432 (0.2432)	Prec@1 91.406 (91.406)
Epoch: [4][60/391]	Time 10.432 (10.398)	Data 0.000 (0.004)	Loss 0.4199 (0.4199)	Prec@1 82.812 (85.246)
Epoch: [4][120/391]	Time 10.359 (10.399)	Data 0.000 (0.002)	Loss 0.3781 (0.4164)	Prec@1 85.156 (85.311)
Epoch: [4][180/391]	Time 10.312 (10.405)	Data 0.000 (0.001)	Loss 0.4524 (0.4214)	Prec@1 85.938 (85.355)
Epoch: [4][240/391]	Time 10.345 (10.405)	Data 0.000 (0.001)	Loss 0.4396 (0.4156)	Prec@1 83.594 (85.536)
Epoch: [4][300/391]	Time 10.452 (10.403)	Data 0.000 (0.001)	Loss 0.3402 (0.4157)	Prec@1 86.719 (85.533)
Epoch: [4][360/391]	Time 10.423 (10.402)	Data 0.000 (0.001)	Loss 0.3587 (0.4177)	Prec@1 85.938 (85.470)
Test: [0/79]	Time 10.550 (10.550)	Loss 0.2435 (0.2435)	Prec@1 91.406 (91.406)
Test: [60/79]	Time 10.334 (10.378)	Loss 0.3259 (0.3735)	Prec@1 90.625 (88.717)
 * Prec@1 88.750
Best Accuracy:89.19
Saving the trained model as: Nov4_retrain_1.th
current lr 1.00000e-04
Epoch: [5][0/391]	Time 10.753 (10.753)	Data 0.225 (0.225)	Loss 0.4767 (0.4767)	Prec@1 86.719 (86.719)
Epoch: [5][60/391]	Time 10.391 (10.415)	Data 0.000 (0.004)	Loss 0.4257 (0.4091)	Prec@1 85.156 (85.950)
Epoch: [5][120/391]	Time 10.453 (10.415)	Data 0.000 (0.002)	Loss 0.3880 (0.4154)	Prec@1 82.812 (85.705)
Epoch: [5][180/391]	Time 10.382 (10.417)	Data 0.000 (0.001)	Loss 0.4706 (0.4169)	Prec@1 85.156 (85.674)
Epoch: [5][240/391]	Time 10.442 (10.419)	Data 0.000 (0.001)	Loss 0.3310 (0.4195)	Prec@1 89.062 (85.571)
Epoch: [5][300/391]	Time 10.478 (10.421)	Data 0.000 (0.001)	Loss 0.4226 (0.4171)	Prec@1 85.156 (85.644)
Epoch: [5][360/391]	Time 10.376 (10.418)	Data 0.000 (0.001)	Loss 0.3662 (0.4189)	Prec@1 85.938 (85.539)
Test: [0/79]	Time 10.710 (10.710)	Loss 0.1950 (0.1950)	Prec@1 95.312 (95.312)
Test: [60/79]	Time 10.399 (10.472)	Loss 0.3141 (0.3821)	Prec@1 91.406 (88.358)
 * Prec@1 88.420
Best Accuracy:89.19
Saving the trained model as: Nov4_retrain_1.th
current lr 1.00000e-04
Epoch: [6][0/391]	Time 10.675 (10.675)	Data 0.223 (0.223)	Loss 0.4191 (0.4191)	Prec@1 83.594 (83.594)
Epoch: [6][60/391]	Time 10.445 (10.492)	Data 0.000 (0.004)	Loss 0.3348 (0.4104)	Prec@1 89.062 (85.374)
Epoch: [6][120/391]	Time 10.405 (10.502)	Data 0.000 (0.002)	Loss 0.3652 (0.4206)	Prec@1 86.719 (85.195)
Epoch: [6][180/391]	Time 10.535 (10.500)	Data 0.000 (0.001)	Loss 0.2750 (0.4263)	Prec@1 91.406 (85.083)
Epoch: [6][240/391]	Time 11.375 (10.508)	Data 0.000 (0.001)	Loss 0.4823 (0.4274)	Prec@1 78.906 (85.111)
Epoch: [6][300/391]	Time 10.513 (10.513)	Data 0.000 (0.001)	Loss 0.3001 (0.4258)	Prec@1 89.062 (85.094)
Epoch: [6][360/391]	Time 10.409 (10.500)	Data 0.000 (0.001)	Loss 0.5523 (0.4250)	Prec@1 83.594 (85.180)
Test: [0/79]	Time 10.668 (10.668)	Loss 0.2602 (0.2602)	Prec@1 93.750 (93.750)
Test: [60/79]	Time 10.491 (10.471)	Loss 0.3022 (0.3846)	Prec@1 90.625 (88.076)
 * Prec@1 88.090
Best Accuracy:89.19
Saving the trained model as: Nov4_retrain_1.th
current lr 1.00000e-04
Epoch: [7][0/391]	Time 11.484 (11.484)	Data 0.264 (0.264)	Loss 0.3628 (0.3628)	Prec@1 85.938 (85.938)
Epoch: [7][60/391]	Time 10.451 (10.448)	Data 0.000 (0.005)	Loss 0.2884 (0.4387)	Prec@1 88.281 (84.580)
Epoch: [7][120/391]	Time 10.357 (10.444)	Data 0.000 (0.002)	Loss 0.5530 (0.4311)	Prec@1 82.812 (85.079)
Epoch: [7][180/391]	Time 10.482 (10.444)	Data 0.000 (0.002)	Loss 0.5055 (0.4337)	Prec@1 82.031 (84.932)
Epoch: [7][240/391]	Time 10.361 (10.446)	Data 0.000 (0.001)	Loss 0.4177 (0.4378)	Prec@1 82.031 (84.832)
Epoch: [7][300/391]	Time 10.369 (10.449)	Data 0.000 (0.001)	Loss 0.3885 (0.4357)	Prec@1 86.719 (84.834)
Epoch: [7][360/391]	Time 10.412 (10.450)	Data 0.000 (0.001)	Loss 0.4079 (0.4368)	Prec@1 86.719 (84.866)
Test: [0/79]	Time 10.508 (10.508)	Loss 0.2579 (0.2579)	Prec@1 90.625 (90.625)
Test: [60/79]	Time 10.132 (10.255)	Loss 0.3713 (0.4041)	Prec@1 88.281 (87.244)
 * Prec@1 87.510
Best Accuracy:89.19
Saving the trained model as: Nov4_retrain_1.th
current lr 1.00000e-04
Epoch: [8][0/391]	Time 10.585 (10.585)	Data 0.241 (0.241)	Loss 0.4061 (0.4061)	Prec@1 82.031 (82.031)
Epoch: [8][60/391]	Time 10.332 (10.282)	Data 0.000 (0.004)	Loss 0.3250 (0.4415)	Prec@1 87.500 (84.721)
Epoch: [8][120/391]	Time 10.281 (10.270)	Data 0.000 (0.002)	Loss 0.3832 (0.4559)	Prec@1 85.156 (84.207)
Epoch: [8][180/391]	Time 10.298 (10.269)	Data 0.000 (0.002)	Loss 0.4592 (0.4572)	Prec@1 85.938 (84.155)
Epoch: [8][240/391]	Time 10.424 (10.275)	Data 0.000 (0.001)	Loss 0.5138 (0.4569)	Prec@1 80.469 (84.190)
Epoch: [8][300/391]	Time 10.355 (10.277)	Data 0.000 (0.001)	Loss 0.5390 (0.4589)	Prec@1 80.469 (84.134)
Epoch: [8][360/391]	Time 10.082 (10.257)	Data 0.000 (0.001)	Loss 0.6087 (0.4681)	Prec@1 77.344 (83.819)
Test: [0/79]	Time 10.416 (10.416)	Loss 0.2735 (0.2735)	Prec@1 92.188 (92.188)
Test: [60/79]	Time 10.141 (10.117)	Loss 0.3922 (0.4186)	Prec@1 90.625 (87.052)
 * Prec@1 87.250
Best Accuracy:89.19
Saving the trained model as: Nov4_retrain_1.th
current lr 1.00000e-04
Epoch: [9][0/391]	Time 10.535 (10.535)	Data 0.224 (0.224)	Loss 0.4373 (0.4373)	Prec@1 81.250 (81.250)
Epoch: [9][60/391]	Time 10.252 (10.269)	Data 0.000 (0.004)	Loss 0.4394 (0.4928)	Prec@1 85.938 (82.864)
Epoch: [9][120/391]	Time 10.406 (10.325)	Data 0.000 (0.002)	Loss 0.5322 (0.4947)	Prec@1 82.812 (82.683)
Epoch: [9][180/391]	Time 10.414 (10.357)	Data 0.000 (0.001)	Loss 0.5733 (0.5010)	Prec@1 79.688 (82.692)
Epoch: [9][240/391]	Time 10.346 (10.383)	Data 0.000 (0.001)	Loss 0.4969 (0.5171)	Prec@1 79.688 (82.342)
Epoch: [9][300/391]	Time 10.388 (10.394)	Data 0.000 (0.001)	Loss 0.6569 (0.5304)	Prec@1 78.906 (81.901)
Epoch: [9][360/391]	Time 10.568 (10.399)	Data 0.000 (0.001)	Loss 0.7805 (0.5436)	Prec@1 76.562 (81.508)
Test: [0/79]	Time 10.589 (10.589)	Loss 0.4187 (0.4187)	Prec@1 87.500 (87.500)
Test: [60/79]	Time 10.338 (10.419)	Loss 0.5602 (0.5669)	Prec@1 85.938 (83.043)
 * Prec@1 82.880
Best Accuracy:89.19
Saving the trained model as: Nov4_retrain_1.th
current lr 1.00000e-05
Epoch: [10][0/391]	Time 10.581 (10.581)	Data 0.196 (0.196)	Loss 0.6825 (0.6825)	Prec@1 76.562 (76.562)
Epoch: [10][60/391]	Time 10.504 (10.445)	Data 0.000 (0.003)	Loss 0.7690 (0.6854)	Prec@1 75.000 (76.614)
Epoch: [10][120/391]	Time 10.426 (10.436)	Data 0.000 (0.002)	Loss 0.5781 (0.6785)	Prec@1 80.469 (76.847)
Epoch: [10][180/391]	Time 10.424 (10.437)	Data 0.000 (0.001)	Loss 0.7426 (0.6886)	Prec@1 76.562 (76.714)
Epoch: [10][240/391]	Time 10.517 (10.438)	Data 0.000 (0.001)	Loss 0.6161 (0.6835)	Prec@1 78.906 (76.841)
Epoch: [10][300/391]	Time 10.374 (10.441)	Data 0.000 (0.001)	Loss 0.6415 (0.6875)	Prec@1 78.906 (76.760)
Epoch: [10][360/391]	Time 10.373 (10.440)	Data 0.000 (0.001)	Loss 0.7575 (0.6897)	Prec@1 78.906 (76.653)
Test: [0/79]	Time 10.659 (10.659)	Loss 0.4988 (0.4988)	Prec@1 85.156 (85.156)
Test: [60/79]	Time 10.341 (10.447)	Loss 0.6263 (0.5993)	Prec@1 83.594 (82.223)
 * Prec@1 82.360
Best Accuracy:89.19
Saving the trained model as: Nov4_retrain_1.th
current lr 1.00000e-05
Epoch: [11][0/391]	Time 10.667 (10.667)	Data 0.188 (0.188)	Loss 0.8614 (0.8614)	Prec@1 69.531 (69.531)
Epoch: [11][60/391]	Time 10.438 (10.441)	Data 0.000 (0.003)	Loss 0.6698 (0.7620)	Prec@1 78.906 (74.885)
Epoch: [11][120/391]	Time 10.488 (10.442)	Data 0.000 (0.002)	Loss 0.6684 (0.7706)	Prec@1 78.125 (74.761)
Epoch: [11][180/391]	Time 10.438 (10.438)	Data 0.000 (0.001)	Loss 0.7576 (0.7612)	Prec@1 73.438 (74.767)
Epoch: [11][240/391]	Time 10.345 (10.437)	Data 0.000 (0.001)	Loss 0.7549 (0.7672)	Prec@1 73.438 (74.559)
Epoch: [11][300/391]	Time 10.564 (10.443)	Data 0.000 (0.001)	Loss 0.7482 (0.7717)	Prec@1 74.219 (74.429)
Epoch: [11][360/391]	Time 10.402 (10.441)	Data 0.000 (0.001)	Loss 0.7489 (0.7845)	Prec@1 73.438 (74.054)
Test: [0/79]	Time 10.598 (10.598)	Loss 0.6789 (0.6789)	Prec@1 75.781 (75.781)
Test: [60/79]	Time 10.252 (10.257)	Loss 0.8084 (0.7048)	Prec@1 77.344 (79.175)
 * Prec@1 79.330
Best Accuracy:89.19
Saving the trained model as: Nov4_retrain_1.th
current lr 1.00000e-05
Epoch: [12][0/391]	Time 10.633 (10.633)	Data 0.197 (0.197)	Loss 1.1984 (1.1984)	Prec@1 66.406 (66.406)
Epoch: [12][60/391]	Time 10.169 (10.272)	Data 0.000 (0.003)	Loss 0.9065 (0.9086)	Prec@1 66.406 (70.966)
Epoch: [12][120/391]	Time 10.292 (10.268)	Data 0.000 (0.002)	Loss 0.7243 (0.9124)	Prec@1 75.000 (71.010)
Epoch: [12][180/391]	Time 10.208 (10.270)	Data 0.000 (0.001)	Loss 1.2565 (0.9418)	Prec@1 64.062 (70.213)
Epoch: [12][240/391]	Time 10.228 (10.270)	Data 0.000 (0.001)	Loss 1.3056 (0.9566)	Prec@1 59.375 (69.891)
Epoch: [12][300/391]	Time 10.331 (10.270)	Data 0.000 (0.001)	Loss 0.9538 (0.9781)	Prec@1 69.531 (69.285)
Epoch: [12][360/391]	Time 10.245 (10.271)	Data 0.000 (0.001)	Loss 1.1245 (0.9999)	Prec@1 63.281 (68.637)
Test: [0/79]	Time 10.739 (10.739)	Loss 1.0062 (1.0062)	Prec@1 67.188 (67.188)
Test: [60/79]	Time 10.348 (10.399)	Loss 1.1083 (0.9695)	Prec@1 73.438 (73.117)
 * Prec@1 73.340
Best Accuracy:89.19
Saving the trained model as: Nov4_retrain_1.th
current lr 1.00000e-05
Epoch: [13][0/391]	Time 10.741 (10.741)	Data 0.200 (0.200)	Loss 0.9760 (0.9760)	Prec@1 68.750 (68.750)
Epoch: [13][60/391]	Time 10.498 (10.387)	Data 0.000 (0.003)	Loss 0.9391 (1.1038)	Prec@1 69.531 (65.868)
Epoch: [13][120/391]	Time 10.299 (10.383)	Data 0.000 (0.002)	Loss 1.1682 (1.1064)	Prec@1 63.281 (65.683)
Epoch: [13][180/391]	Time 10.311 (10.385)	Data 0.000 (0.001)	Loss 1.0083 (1.1172)	Prec@1 69.531 (65.383)
Epoch: [13][240/391]	Time 10.378 (10.395)	Data 0.000 (0.001)	Loss 1.1857 (1.1338)	Prec@1 60.938 (65.032)
Epoch: [13][300/391]	Time 10.550 (10.404)	Data 0.000 (0.001)	Loss 1.4628 (1.1489)	Prec@1 59.375 (64.618)
Epoch: [13][360/391]	Time 10.366 (10.410)	Data 0.000 (0.001)	Loss 1.3992 (1.1715)	Prec@1 61.719 (64.080)
Test: [0/79]	Time 10.490 (10.490)	Loss 1.4508 (1.4508)	Prec@1 66.406 (66.406)
Test: [60/79]	Time 10.342 (10.283)	Loss 1.4485 (1.2678)	Prec@1 67.188 (66.586)
 * Prec@1 66.590
Best Accuracy:89.19
Saving the trained model as: Nov4_retrain_1.th
current lr 1.00000e-06
Epoch: [14][0/391]	Time 10.619 (10.619)	Data 0.220 (0.220)	Loss 1.2714 (1.2714)	Prec@1 58.594 (58.594)
Epoch: [14][60/391]	Time 10.291 (10.267)	Data 0.000 (0.004)	Loss 1.6509 (1.4186)	Prec@1 44.531 (58.184)
Epoch: [14][120/391]	Time 10.245 (10.266)	Data 0.000 (0.002)	Loss 1.5097 (1.4310)	Prec@1 58.594 (58.077)
Epoch: [14][180/391]	Time 10.347 (10.270)	Data 0.000 (0.001)	Loss 1.4610 (1.4246)	Prec@1 59.375 (58.322)
Epoch: [14][240/391]	Time 10.216 (10.270)	Data 0.000 (0.001)	Loss 1.4005 (1.4123)	Prec@1 53.906 (58.396)
Epoch: [14][300/391]	Time 10.193 (10.274)	Data 0.000 (0.001)	Loss 1.3087 (1.4021)	Prec@1 57.812 (58.430)
Epoch: [14][360/391]	Time 10.373 (10.274)	Data 0.000 (0.001)	Loss 1.5004 (1.3940)	Prec@1 57.031 (58.605)
Test: [0/79]	Time 10.538 (10.538)	Loss 1.3737 (1.3737)	Prec@1 64.844 (64.844)
Test: [60/79]	Time 10.380 (10.255)	Loss 1.2842 (1.1767)	Prec@1 69.531 (68.225)
 * Prec@1 68.250
Best Accuracy:89.19
Saving the trained model as: Nov4_retrain_1.th
